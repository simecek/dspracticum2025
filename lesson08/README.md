**Date**: Nov 3, 2025

**Slides**: https://docs.google.com/presentation/d/146v_p2yg8QAEVuISd8fXBeZBlYOlAXtqopozRkeZe4E/edit?usp=sharing

* LLMs, both open and commercial, providers, benchmaks
* ChatGT API
* LoRA & other tricks

**Additional materials:**

* [Intro to Quantization in LLMs](https://medium.com/@prathamgrover777/intro-to-quantization-in-llms-adfae03bf447)
* [KV Caching & Attention Optimization](https://medium.com/@prathamgrover777/kv-caching-attention-optimization-from-o-n%C2%B2-to-o-n-8b605f0d4072)
* Sebastian Raschka: [From GPT-2 to gpt-oss: Analyzing the Architectural Advances](https://sebastianraschka.com/blog/2025/from-gpt-2-to-gpt-oss.html)

**Assignment 08** (due **Nov 10, 8:30 AM**)

Experiment with the [UnSloth Colabs](https://github.com/unslothai/notebooks) and try to make something **useful or funny**.

**Examples:**

- [Fine-tune an LLM](https://docs.unsloth.ai/basics/continued-pretraining) on your own text corpus (the same one you used for GPT-2 training). Is it better than your character-level GPT-2?
- Use the ChatGPT API to create a small **classification dataset** (e.g. 100 examples for training and 100 for testing, similar to last yearâ€™s homework). Then fine-tune **Gemma 3** on the training set and evaluate it on the test set.
- Take a model that is currently **not included in MiniCzechBenchmark** and measure its performance on that benchmark.

Submit your notebook via the form (link on Slack).

