**Date**: Oct 13, 2025

**Slides**: https://docs.google.com/presentation/d/1Yro7-_ihRKAPVjSVGTVNbYS9wT7DGwH4TqyYwBnbYYc/edit?usp=sharing

* Building blocks (`torch.nn.Module`)
* Hugging Face `datasets`
* Hugging Face `transformers`

**Additional materials:**

* [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
* [BERT paper (Pre-training of Deep Bidirectional Transformers for Language Understanding)](https://arxiv.org/abs/1810.04805)
* [GPT-2 paper (Language Models are Unsupervised Multitask Learners)](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf)
* [GPT-3 paper (Language Models are Few-Shot Learners)](https://arxiv.org/abs/2005.14165)
* [Stanford: Building Large Language Models (LLMs)](https://www.youtube.com/watch?v=9vM4p9NN0Ts)

**Assignment 05** (due to Oct 27, 8:30 AM):

Find and download text that can be used to train or fine-tune a GPT-2 model.
The text should contain between **1â€“10 million characters** (if more, you can later use a sample).
It can be in any language, but Czech or Slovak is preferable.
It must be **your own new idea**, not just a copy of existing datasets.
Link for submissions is in Slack.

**Inspiration**

* Public government or parliament documents (e.g., speeches, presidential addresses, etc.)
* Judge rulings, academic senate recordings
* Wikipedia pages, old religious texts
* Books in the public domain
* Subtitles, lyrics, poetry
* Even non-human languages (DNA, MIDI music), though tokenization might be more challenging